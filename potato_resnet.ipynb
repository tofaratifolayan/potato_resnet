{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tofaratifolayan/potato_resnet/blob/main/potato_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C6JV2NI1ClG",
        "outputId": "d2a0368f-b679-4ff6-8e9d-08ee787cb086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  799M  100  799M    0     0  20.5M      0  0:00:38  0:00:38 --:--:-- 22.9M\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o potato-viral-disease-dataset.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/nirmalsankalana/potato-viral-disease-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q potato-viral-disease-dataset.zip -d potato-viral-disease-dataset"
      ],
      "metadata": {
        "id": "VOwAXeRS1DiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o potato-healthy-dataset.zip\\\n",
        "https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5m38z6jthb-1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEl6ogoV1FVg",
        "outputId": "928dfef9-843e-4a65-cc84-78ba6773a4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  238M  100  238M    0     0  14.3M      0  0:00:16  0:00:16 --:--:-- 16.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q potato-healthy-dataset.zip -d potato-viral-disease-dataset"
      ],
      "metadata": {
        "id": "BpuuMnTv1Jof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "class_dict = {\"Potato___healthy\": 0, \"Potato___leafroll_virus\": 1, \"Potato___mosaic_virus\": 2, \"Potato___spindle_tuber_viroid\": 3, \"Potato__healthy_bulb\": 4}\n",
        "class_folders = os.listdir(\"/content/potato-viral-disease-dataset\")\n",
        "# Iterate over each class folder\n",
        "for label, class_name in enumerate(class_folders):\n",
        "    class_folder = os.path.join(\"/content/potato-viral-disease-dataset\", class_name)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(class_folder):\n",
        "        for file_name in os.listdir(class_folder):\n",
        "            file_path = os.path.join(class_folder, file_name)\n",
        "            if not os.path.isdir(file_path):\n",
        "              file_paths.append(file_path)\n",
        "              # Store class name as label\n",
        "              labels.append(class_dict[class_name])\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\"file_path\": file_paths, \"label\": labels})\n",
        "\n",
        "# Display the first few rows\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcjX93Sj1LFY",
        "outputId": "d4a40fef-b4e9-4a5e-f4ad-03249d88c214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              file_path  label\n",
            "0     /content/potato-viral-disease-dataset/Potato__...      3\n",
            "1     /content/potato-viral-disease-dataset/Potato__...      3\n",
            "2     /content/potato-viral-disease-dataset/Potato__...      3\n",
            "3     /content/potato-viral-disease-dataset/Potato__...      3\n",
            "4     /content/potato-viral-disease-dataset/Potato__...      3\n",
            "...                                                 ...    ...\n",
            "1558  /content/potato-viral-disease-dataset/Potato__...      2\n",
            "1559  /content/potato-viral-disease-dataset/Potato__...      2\n",
            "1560  /content/potato-viral-disease-dataset/Potato__...      2\n",
            "1561  /content/potato-viral-disease-dataset/Potato__...      2\n",
            "1562  /content/potato-viral-disease-dataset/Potato__...      2\n",
            "\n",
            "[1563 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    # Resize smaller edge to 256\n",
        "    transforms.Resize(256),\n",
        "    # Crop the center to 224x224\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = hw dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx, 0]\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Ensure it's a tensor\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "00VfgTDY1SLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maU3o0aP1skc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, numChannels, out_channels, third_conv=False, downsample=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # conv1\n",
        "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=out_channels, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # conv2\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # relu\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # 3rd conv\n",
        "        self.third_conv = third_conv\n",
        "        if third_conv:\n",
        "            self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1)\n",
        "            self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "\n",
        "        self.downsample = nn.Conv2d(numChannels, out_channels, kernel_size=1, stride=1, bias=False) if numChannels != out_channels else None\n",
        "\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.third_conv:\n",
        "            Y = self.bn3(self.conv3(self.relu(Y)))\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        Y += identity\n",
        "        return self.relu(Y)\n",
        "\n",
        "class PotatoNet(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.residual1 = Residual(64, 64)\n",
        "        self.residual2 = Residual(64, 128, downsample=False)\n",
        "        self.residual3 = Residual(128, 256, downsample=False)\n",
        "        self.residual4 = Residual(256, 512, downsample=False)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.last_fc = nn.Linear(4096, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.relu(self.bn1(self.conv1(X)))\n",
        "        X = self.pool(X)\n",
        "\n",
        "        X = self.residual1(X)\n",
        "        X = self.residual2(X)\n",
        "        X = self.residual3(X)\n",
        "        X = self.residual4(X)\n",
        "\n",
        "        X = self.global_pool(X)\n",
        "        X = torch.flatten(X, 1)\n",
        "\n",
        "        X = self.fc1(X)\n",
        "        X = self.relu(X)\n",
        "        X = self.fc2(X)\n",
        "        X = self.relu(X)\n",
        "        X = self.last_fc(X)\n",
        "\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "rL8aJFcc2u2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "train_dataset = ImageDataset(train_df, transform=transform)\n",
        "test_dataset = ImageDataset(test_df, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "il_kkmhT1mbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            progress_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss/len(train_loader):.4f}, Accuracy = {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-OgilTW57pTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to device\n",
        "model = PotatoNet(num_classes=5).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Attach scheduler to optimizer\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# def train\n",
        "def train(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    best_acc = 0  # Store best accuracy\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            progress_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss/len(train_loader):.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "        # Adjust learning rate based on validation accuracy\n",
        "\n",
        "        # **Save Best Model**\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Best Model Saved at {best_model_path} with Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Training Complete! Best Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "# Train the model\n",
        "train(model, train_dataloader, criterion, optimizer, device, num_epochs=10)\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_dataloader, criterion, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzNpDTUBo0V",
        "outputId": "02bd67fd-e9a7-4376-c082-c1fa546a34af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 20/20 [00:43<00:00,  2.16s/it, acc=63.2, loss=0.457]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.9682, Accuracy = 63.20%\n",
            "Best Model Saved at best_model.pth with Accuracy: 63.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 20/20 [00:42<00:00,  2.14s/it, acc=85.4, loss=0.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss = 0.3798, Accuracy = 85.36%\n",
            "Best Model Saved at best_model.pth with Accuracy: 85.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 20/20 [00:42<00:00,  2.15s/it, acc=84.4, loss=0.218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss = 0.3783, Accuracy = 84.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 20/20 [00:43<00:00,  2.16s/it, acc=90, loss=0.463]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss = 0.2670, Accuracy = 90.00%\n",
            "Best Model Saved at best_model.pth with Accuracy: 90.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 20/20 [00:43<00:00,  2.17s/it, acc=88.3, loss=0.394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss = 0.3054, Accuracy = 88.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 20/20 [00:42<00:00,  2.15s/it, acc=88.6, loss=0.191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Loss = 0.2904, Accuracy = 88.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 20/20 [00:42<00:00,  2.13s/it, acc=89.7, loss=0.382]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Loss = 0.2564, Accuracy = 89.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 20/20 [00:42<00:00,  2.12s/it, acc=90.7, loss=0.289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Loss = 0.2332, Accuracy = 90.72%\n",
            "Best Model Saved at best_model.pth with Accuracy: 90.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 20/20 [00:42<00:00,  2.12s/it, acc=90.3, loss=0.334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Loss = 0.2515, Accuracy = 90.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 20/20 [00:43<00:00,  2.18s/it, acc=87.8, loss=0.414]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Loss = 0.2742, Accuracy = 87.84%\n",
            "Training Complete! Best Accuracy: 90.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2279, Test Accuracy: 90.73%\n"
          ]
        }
      ]
    }
  ]
}